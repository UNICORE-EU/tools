#!/bin/bash

_munge_setup() {
    id
    mkdir -p /run/munge
    chown munge:munge /run/munge
    sudo -u munge munged
}

_slurm_setup() {

    /usr/sbin/groupadd -r slurm 2>/dev/null    
    /usr/sbin/useradd -c "Slurm" -g slurm \
      -s /bin/false -r -d /tmp slurm 2>/dev/null

    mkdir -p /var/run/slurm
    chown slurm:slurm /var/run/slurm
    mkdir -p /var/spool/slurm
    chown slurm:slurm /var/spool/slurm

    cat > /etc/slurm/slurm.conf <<EOF

SlurmctldHost=localhost
AuthType=auth/munge
EnforcePartLimits=NO
ProctrackType=proctrack/pgid
ReturnToService=0
SlurmctldPidFile=/var/run/slurm/slurmctld.pid
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurm/slurmd.pid
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/d
SlurmUser=slurm
SlurmdUser=root
StateSaveLocation=/var/spool/slurm/ctld
SwitchType=switch/none
TaskPlugin=task/none

# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/linear

AccountingStorageType=accounting_storage/none
ClusterName=testcluster
JobCompType=jobcomp/none
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=3
SlurmdDebug=3
MailProg=/bin/true

# COMPUTE NODES
NodeName=$(hostname -s) CPUs=$(nproc) State=UNKNOWN
PartitionName=batch Nodes=$(hostname -s) Default=YES MaxTime=INFINITE State=UP OverSubscribe=FORCE
EOF
    echo "    ... starting slurmctld."
    sudo -u slurm slurmctld
    echo "    ... starting slurmd."
    slurmd
}

_users_setup() {
    /usr/sbin/useradd -c "Demouser" -g users \
      -s /bin/bash -r -b /home demouser 2>/dev/null
    mkdir -p /home/demouser
    chown demouser:users /home/demouser
}

_unicore_setup() {
    /usr/sbin/groupadd -r unicore 2>/dev/null    
    /usr/sbin/useradd -c "UNICORE" -g unicore \
      -s /bin/false -r -d /tmp unicore 2>/dev/null

    mkdir -p /opt/unicore
    chown unicore:unicore /opt/unicore
    cd /opt/unicore
    mv /tmp/unicore-servers.tgz /opt/unicore
    chown unicore:unicore /opt/unicore/unicore-servers.tgz
    sudo -u unicore tar xzf unicore-servers.tgz
    rm unicore-servers.tgz
    sudo -u unicore mv unicore-servers-* unicore-servers

    # Services
    mv /tmp/configure.properties /opt/unicore/unicore-servers
    chown unicore:unicore /opt/unicore/unicore-servers/configure.properties
    cd /opt/unicore/unicore-servers
    sudo -u unicore python3 ./configure.py
    sudo -u unicore python3 ./install.py

    # configure user mapping so jobs run as "demouser"
    sudo -u unicore cat > /opt/unicore/unicore-servers/unicorex/conf/simpleuudb <<EOF
{
  "CN=Demo User, O=UNICORE, C=EU" : {
    "role": "user",
    "xlogin": "demouser"
  }
}
EOF

    # TSI for Slurm
    cd /opt/unicore/unicore-servers/tsi
    sudo -u unicore ./Install.sh slurm /opt/unicore/tsi_slurm
    cd /opt/unicore
    cat > /opt/unicore/tsi_slurm/conf/tsi.properties <<EOF

# (autogenerated)
tsi.unicorex_machine=localhost,dockerhost
tsi.unicorex_port=7654
tsi.my_addr=0.0.0.0
tsi.my_port=4433
tsi.disable_ipv6=1
tsi.enforce_gids_consistency=true
tsi.fail_on_invalid_gids=false
tsi.default_job_name=UnicoreJob
tsi.setfacl=setfacl
tsi.getfacl=getfacl
tsi.acl./=NONE
tsi.usersCacheTtl=600
tsi.safe_dir=/tmp
EOF
    echo "    ... starting TSI."
    /opt/unicore/tsi_slurm/bin/start.sh
    if [[ "${TSI_ONLY}" != "" ]] ; then
	return
    else
	echo "    ... starting UNICORE services."
    fi
    sudo -u unicore /opt/unicore/unicore-servers/start.sh
}

_main() {
    echo "Creating user..."
    _users_setup
    echo "Setting up Slurm..."
    _munge_setup
    _slurm_setup
    echo "Setting up UNICORE..."
    _unicore_setup

    if [[ -t 0 && -t 1 ]] ; then
	echo "Running interactive shell"
	if [[ "${1:0:1}" = "-" ]]; then
            echo "Please pass a program name to the container!"
            exit 1
	else
            exec "$@"
	fi
    else
	echo "Running detached"
	# just keep the services running
	tail -f /opt/unicore/tsi_slurm/logs/TSI*
    fi
}

_main "$@"

